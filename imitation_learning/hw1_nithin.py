

from collections import OrderedDict 
import gym
import keras
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy as np
import subprocess
import random
import tensorflow as tf

"""### Make the TF Model
We'll use the same architecture for each of the problems. By implementing a function that creates the model here, you won't need to implement it again for each problem.
"""

def make_model():
  model = Sequential()      
  
  model.add(Dense(10, activation='tanh', use_bias = True, input_dim=4))
  model.add(Dense(2, activation='softmax', use_bias = True))

  model.compile(loss='categorical_crossentropy',
                     optimizer=tf.train.AdamOptimizer(),
                     metrics=['accuracy'])
  
  assert len(model.weights) == 4, 'Model should have 4 weights.'
  return model


model = make_model()

for t in range(20):
  X = np.random.normal(size=(1000, 4))  # some random data
  is_positive = np.sum(X, axis=1) > 0  # A simple binary function
  Y = np.zeros((1000, 2))
  Y[np.arange(1000), is_positive.astype(int)] = 1  # one-hot labels
  history = model.fit(X, Y, epochs=10, batch_size=256, verbose=0)
  loss = history.history['loss'][-1]
  acc = history.history['acc'][-1]
  print('(%d) loss= %.3f; accuracy = %.1f%%' % (t, loss, 100 * acc))



def action_to_one_hot(env, action):
    action_vec = np.zeros(env.action_space.n)
    action_vec[action] = 1
    return action_vec    
      
# Interacting with the Gym 
def generate_episode(env, policy):
  """Collects one rollout from the policy in an environment. The environment
  should implement the OpenAI Gym interface. A rollout ends when done=True. The
  number of states and actions should be the same, so you should not include
  the final state when done=True.

  Args:
    env: an OpenAI Gym environment.
    policy: a keras model
  Returns:
    states: a list of states visited by the agent.
    actions: a list of actions taken by the agent. While the original actions
      are discrete, it will be helpful to use a one-hot encoding. The actions
      that you return should be one-hot vectors (use action_to_one_hot())
    rewards: the reward received by the agent at each step.
  """
  done = False
  state = env.reset()

  states = []
  actions = []
  rewards = []
    
  while not done:
    
    states.append(state.reshape(1,4))

    action = policy.predict(state.reshape(1,4))  
    action_id = np.argmax(action)
    action = action_to_one_hot(env, action_id)
    state, reward, done, info = env.step(action_id)
    
      
    actions.append(action)
    rewards.append(reward)
    
  return np.array(states), np.array(actions), np.array(rewards)


# Create the environment.
env = gym.make('CartPole-v0')
policy = make_model()
states, actions, rewards = generate_episode(env, policy)
assert len(states) == len(actions), 'Number of states and actions should be equal.'
assert len(actions) == len(rewards), 'Number of actions and rewards should be equal.'
assert len(actions[0]) == 2, 'Actions should use one-hot encoding.'
print('Test passed!')


ok = (subprocess.check_output(['md5sum', 'expert.h5']).split()[0] == b'3f465e682aa859858cf5951f04a91677')
print('Status: %s' % 'OK' if ok else 'FAILED')


# Implementing Behavior Cloning and DAGGER

class Imitation():

    def __init__(self, env, num_episodes):
        self.env = env
        self.expert = tf.keras.models.load_model('expert.h5')
        self.num_episodes = num_episodes
        self.model = make_model()
        
    def generate_behavior_cloning_data(self):
        self._train_states = []
        self._train_actions = []

        for _ in range(self.num_episodes):
            states, actions, rewards = generate_episode(self.env, self.expert)
            self._train_states.extend(states)
            self._train_actions.extend(actions)

        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def generate_dagger_data(self):
        
        self._train_states = []
        self._train_actions = []

        for _ in range(self.num_episodes):
            states, actions, rewards = generate_episode(self.env, self.model)
            self._train_states.extend(states)
        self._train_states = np.array(self._train_states)

        for state in self._train_states:
            expert_action = self.expert.predict(state)
            expert_action_argmax = np.argmax(expert_action)
            expert_one_hot_action = action_to_one_hot(env, expert_action_argmax)
            self._train_actions.append(expert_one_hot_action)
            
        self._train_actions = np.array(self._train_actions)        
        
    def train(self, num_epochs=200, render=False):
        """Trains the model on training data generated by the expert policy.
        Args:
          env: The environment to run the expert policy on.
          num_epochs: number of epochs to train on the data generated by the expert.
          render: Whether to render the environment.
        Return:
          loss: (float) final loss of the trained policy.
          acc: (float) final accuracy of the trained policy
        """        
        X = self._train_states
        Y = self._train_actions
        print(X.shape)
        print(Y.shape)
        
        history = self.model.fit(X.squeeze(1), Y, epochs=num_epochs, batch_size=256, verbose=0)
        loss = history.history['loss'][-1]
        acc = history.history['acc'][-1]
        
        return loss, acc


    def evaluate(self, policy, n_episodes=50):
        rewards = []
        for i in range(n_episodes):
            _, _, r = generate_episode(self.env, policy)
            rewards.append(sum(r))
        r_mean = np.mean(rewards)
        return r_mean


# mode = 'behavior cloning'
mode = 'dagger'

num_episodes = 100
                    
num_iterations = 100  

# Create the environment.
env = gym.make('CartPole-v0')
im = Imitation(env, num_episodes)
expert_reward = im.evaluate(im.expert)
print('Expert reward: %.2f' % expert_reward)

loss_vec = []
acc_vec = []
imitation_reward_vec = []
for t in range(num_iterations):
  if mode == 'behavior cloning':
    im.generate_behavior_cloning_data()
  elif mode == 'dagger':
    im.generate_dagger_data()
  else:
    raise ValueError('Unknown mode: %s' % mode)
  loss, acc = im.train(num_epochs=1)
  imitation_reward = im.evaluate(im.model)
  loss_vec.append(loss)
  acc_vec.append(acc)
  imitation_reward_vec.append(imitation_reward)
  print('(%d) loss = %.3f; accuracy = %.2f; reward = %.1f' % (t, loss, acc, imitation_reward))


### Plot the results
plt.figure(figsize=(12, 3))
plt.subplot(131)
plt.title('Reward')
plt.plot(imitation_reward_vec, label='imitation')
plt.hlines(expert_reward, 0, len(imitation_reward_vec), label='expert')
plt.xlabel('iterations')
plt.ylabel('return')
plt.legend()
plt.ylim([0, None])

plt.subplot(132)
plt.title('Loss')
plt.plot(loss_vec)
plt.xlabel('iterations')
plt.ylabel('loss')

plt.subplot(133)
plt.title('Accuracy')
plt.plot(acc_vec)
plt.xlabel('iterations')
plt.ylabel('accuracy')
plt.tight_layout()
plt.savefig('student_vs_expert_%s.png' % mode, dpi=300)
plt.show()


random_seeds = 5
# Dictionary mapping number of expert trajectories to a list of rewards.
# Each is the result of running with a different random seed.
reward_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})
accuracy_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})
loss_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})
for num_episodes in [1, 10, 50, 100]:
  for e in range(random_seeds):
    print('num_episodes: %s; seed: %d' % (num_episodes, e))
    
    # mode = 'behavior cloning'
    mode = 'dagger'

    num_iterations = 200  

    # Create the environment.
    env = gym.make('CartPole-v0')
    im = Imitation(env, num_episodes)
    expert_reward = im.evaluate(im.expert)
    print('Expert reward: %.2f' % expert_reward)

    loss_vec = []
    acc_vec = []
    imitation_reward_vec = []
    for t in range(num_iterations):
      if mode == 'behavior cloning':
        im.generate_behavior_cloning_data()
      elif mode == 'dagger':
        im.generate_dagger_data()
      else:
        raise ValueError('Unknown mode: %s' % mode)
      loss, acc = im.train(num_epochs=1)
      imitation_reward = im.evaluate(im.model)
      loss_vec.append(loss)
      acc_vec.append(acc)
      imitation_reward_vec.append(imitation_reward)
      print('(%d) loss = %.3f; accuracy = %.2f; reward = %.1f' % (t, loss, acc, imitation_reward))
      
    reward_data[num_episodes].append(imitation_reward_vec)
    accuracy_data[num_episodes].append(acc_vec)
    loss_data[num_episodes].append(loss_vec)

"""Plot the reward, loss, and accuracy for each, remembering to label each line."""

keys = [1, 10, 50, 100]
plt.figure(figsize=(12, 4))
for (index, (data, name)) in enumerate(zip([reward_data, accuracy_data, loss_data],
                                           ['reward', 'accuracy', 'loss'])):
  plt.subplot(1, 3, index + 1)
  plt.plot(data.keys(), data.values(), '-o')
  plt.xlabel('number of expert trajectories', fontsize=16)
  plt.ylabel('name', fontsize=16)
plt.savefig('expert_data_%s.png' % mode, dpi=300)
plt.show()

# Problem 2: CMA-ES


class CMAES:
    def __init__(self, env, L, n, p, sigma, noise, reward_fn=None):
        """
        Args:
          env: environment with which to evaluate different weights
          L: number of episodes used to evaluate a given set of weights
          n: number of members (weights) in each generation
          p: proportion of members used to update the mean and covariance
          sigma: initial std
          noise: additional noise to add to covariance
          reward_fn: if specified, this reward function is used instead of the 
            default environment reward. This will be used in Problem 3, when the
            reward function will come from the discriminator. The reward
            function should be a function of the state and action.
        """

        self.env = env
        self.model = make_model()
        # Let d be the dimension of the 1d vector of weights.
        self.d = sum(int(np.product(w.shape)) for w in self.model.weights)
        self.mu = np.zeros(self.d)
        self.S = sigma**2 * np.eye(self.d)

        self.L = L
        self.n = n
        self.p = p
        self.noise = noise
        self.reward_fn = reward_fn


    def populate(self):
        """
        Populate a generation using the current estimates of mu and S
        """
        self.population = np.random.multivariate_normal(self.mu, self.S, self.n)
        self.population = list(self.population)        


    def set_weight(self, member):
        ind = 0
        weights = []
        for w in self.model.weights:
          if len(w.shape) > 1:
            mat = member[ind:ind+int(w.shape[0]*w.shape[1])]
            mat = np.reshape(mat, w.shape)
            ind += int(w.shape[0]*w.shape[1])
          else:
            mat = member[ind:ind+int(w.shape[0])]
            ind += int(w.shape[0])
          weights.append(mat)

        self.model.set_weights(weights)
        

    def evaluate(self, member, num_episodes):
        """
        Evaluate a set of weights by interacting with the environment and
        return the average total reward over num_episodes.
        """
        self.set_weight(member)
        return self.evaluate_policy(self.model, num_episodes)
    
    def evaluate_policy(self, policy, num_episodes):
        episode_rewards = []
        for episode in range(num_episodes):
          _, _, r = generate_episode(self.env, policy)
          episode_rewards.append(sum(r))

        return np.mean(episode_rewards)
    
    def train(self):
        """
        Perform an iteration of CMA-ES by evaluating all members of the
        current population and then updating mu and S with the top self.p
        proportion of members. Note that self.populate automatically deletes
        all the old members, so you don't need to worry about deleting the
        "bad" members.

        """
        self.populate()
        
        member_rewards = []
        for i in self.population:
          member_rewards.append(self.evaluate(i, self.L))
        
        member_rewards = np.array(member_rewards)
        proportion_no = round(self.p * self.n)
        top_idx = np.argsort(-member_rewards)[:proportion_no]

        new_generation = []

        for i in top_idx:
          new_generation.append(self.population[i])
        
        new_generation = np.vstack(new_generation)

        self.mu = np.mean(new_generation, axis = 0)
        self.S = self.noise * np.eye(self.mu.shape[0]) + np.cov(new_generation, rowvar = False)
        
        best_member = new_generation[0]
        best_r = self.evaluate(best_member, 10)
        mu_r = self.evaluate(self.mu, 10)
      
        return mu_r, best_r

iterations = 200  
pop_size_vec = [50]             
# pop_size_vec = [20, 50, 100]  
                              
data = {pop_size: [] for pop_size in pop_size_vec}

for pop_size in pop_size_vec:
  print('Population size:', pop_size)
  env = gym.make('CartPole-v0')
  optimizer = CMAES(env,
                    L=1,  # number of episodes for evaluation
                    n=pop_size,  # population size
                    p=0.25,  # proportion of population to keep
                    sigma=10,  # initial std dev
                    noise=0.25)  # noise

  for t in range(iterations):
      mu_r, best_r = optimizer.train()
      data[pop_size].append((mu_r, best_r))
      print('(%d) avg member rew = %.2f; best member rew = %.2f' % (t, mu_r, best_r))

plt.figure(figsize=(12, 4))
plt.subplot(121)  # Left plot will show performance vs number of iterations
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 1]  # Use the performance of the best point
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(pop_size))
  plt.ylabel('average return', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)

plt.subplot(122)  # Right plot will show performance vs number of points evaluated
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 1]  # Use the performance of the best point
  x = pop_size * (np.arange(len(mu_r)) + 1)
  plt.plot(x, mu_r, label=str(pop_size))
  plt.ylabel('average return', fontsize=16)
  plt.xlabel('num. points evaluated', fontsize=16)

plt.legend()
plt.tight_layout()
plt.savefig('cmaes_pop_size.png')
plt.show()

# General Adversarial Imitation Learning (GAIL)
class GAIL(object):
  
  def __init__(self, env):
    self.env = env
    self.expert = tf.keras.models.load_model('expert.h5')
    # self.model = make_model()
    self.discriminator = make_model()
    self.cmaes = CMAES(env,
                  L=1,  # number of episodes for evaluation
                  n=20,  # population size
                  p=0.25,  # proportion of population to keep
                  sigma=10,  # initial std dev
                  noise=0.25,  # noise
                  reward_fn=self._reward_fn)
    self.expert_states = []
    self.student_states = []
    
  def _reward_fn(self, s, a):
    """Log probability that state is from expert."""
    del a
    p_expert = self.discriminator.predict(s[None])[0][0]
    return np.log(p_expert)
    
  def collect_data(self, num_episodes):
    """Collect data from the expert and imitation policy. After the initial
    iteration, there is no need to collect new data from the expert, as the
    expert policy never changes.
    """
    collect_expert = len(self.expert_states) == 0
    
    for _ in range(num_episodes):

      # Collect data from the expert policy
      if collect_expert == 1:
        states, actions, rewards = generate_episode(self.env, self.expert)
        self.expert_states.extend(states)
      
      # Collect data from the student policy
      states, actions, rewards = generate_episode(self.env, self.cmaes.model)
      self.student_states.extend(states)
    
    # self.expert_states = np.array(self.expert_states)
    # self.student_states = np.array(self.student_states)

  def train_discriminator(self):

    training_states = list(self.expert_states)
    training_states.extend(self.student_states)
    
    # Stack up the expert and student states into one variable for training
    X = np.vstack(training_states)

    # Create  labels for the states 
    Y = np.repeat(np.array([[1,0],[0,1]]), [len(self.expert_states), len(self.student_states)], axis = 0)

    assert Y.shape[1] == 2  # Use a 1-hot encoding for the labels
    assert np.all(np.sum(Y, axis=1) == 1)
    history = self.discriminator.fit(X, Y, epochs=10, batch_size=256, verbose=0)
    loss = history.history['loss'][-1]
    acc = history.history['acc'][-1]
    return loss, acc
  
  def train_policy(self):
    mu_r, best_r = self.cmaes.train()
    return mu_r, best_r

### Implement the total variation distance to compare two policies
def get_x_position_histogram(states):
  x_vec = [s[0] for s in states]  # The x position is the first coordinate
  bins = np.linspace(-2.4, 2.4, 11)  # Need 11 edges to make 10 bins
  hist, _ = np.histogram(x_vec, bins=bins, density=True)
  return hist

def TV_distance(expert_states, student_states):
  expert_hist = get_x_position_histogram(expert_states)
  student_hist = get_x_position_histogram(student_states)
  return 0.5 * np.sum(np.abs(expert_hist - student_hist))

def evaluate(gail):
  """Evaluate the policy learned by GAIL according to three metrics:
    1. Environment reward. We want this number to be large (~100)
    2. How well it fools the discriminator. In particular, we compute the
      discriminator's prediction that the policy is the expert. The policy
      tries to increase this number, while the discriminator tries to decrease
      it. We expect it to be around 40% - 60%
    3. Total variation distance between the student and the expert, along the
      X axis. We want this number to be small (~0)."""
  rewards_vec = []
  p_expert_vec = []
  for _ in range(10):
    states, actions, rewards = generate_episode(gail.env, gail.cmaes.model)
    states = states.squeeze(1)
    rewards_vec.append(np.sum(rewards))
    log_p_expert = [gail._reward_fn(s, a) for (s, a) in zip(states, actions)]
    p_expert = np.exp(log_p_expert)
    p_expert_vec.append(np.mean(p_expert))
  tv_dist = TV_distance(gail.expert_states, gail.student_states)
  return np.mean(p_expert_vec), np.mean(rewards_vec), tv_dist

discriminator_update_period = [10, 20]

for dis_no in discriminator_update_period:

  env = gym.make('CartPole-v0')
  gail = GAIL(env)

  num_episodes = 20      #increase later
  loss_vec = []
  acc_vec = []
  itr = []
  reward_vec = []
  tv_dist_vec = []

  for t in range(100):
    # WRITE CODE HERE
    gail.collect_data(num_episodes)
    gail.train_policy()
    
    # if(t % discriminator_update_period == 0):
    if(t % dis_no == 0):
      loss, acc = gail.train_discriminator()

    p_expert, avg_r, tv_dist = evaluate(gail)
    print('(%d) Policy: p(expert) = %.2f%% ; reward = %.1f' % (t, 100.0 * p_expert, avg_r))
    print('(%d) Total variation distance = %.2f' % (t, tv_dist))
    loss_vec.append(loss)
    acc_vec.append(acc)
    itr.append(t)
    reward_vec.append(avg_r)
    tv_dist_vec.append(tv_dist)

  '''
  plt.figure()
  plt.plot(itr, acc_vec)
  plt.ylabel('accuracy', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.savefig('3.1.png')
  plt.show()
  '''

  plt.figure(figsize=(12, 4))
  plt.subplot(121)
  plt.plot(itr, reward_vec)
  plt.ylabel('reward', fontsize=16)
  plt.xlabel('num iterations', fontsize=16)

  plt.subplot(122)
  plt.plot(itr, tv_dist_vec)
  plt.ylabel('TV distance', fontsize=16)
  plt.xlabel('num iterations', fontsize=16)

  plt.savefig('3.4.{}.png'.format(dis_no))
  plt.show()
